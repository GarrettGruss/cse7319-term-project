{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffdcde6b",
   "metadata": {},
   "source": [
    "Based on the following video: [How-to Use The Reddit API in Python by James Briggs](https://www.youtube.com/watch?v=FdjVoOf9HN4)\n",
    "1. Create a reddit application [here](https://www.reddit.com/prefs/apps) to get a client_id and client_secret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b8e12",
   "metadata": {},
   "source": [
    "# Example workflow\n",
    "\n",
    "## Step 1: Find a Trending Post\n",
    "Use various sorting techniques such as hot, trending, controversial to select a post with insightful information. [Example post](https://www.reddit.com/r/mcp/comments/1nculrw/why_are_mcps_needed_for_basic_tools_like/)\n",
    "\n",
    "## Step 2: Convert the Post Into a YAML Summary\n",
    "Apply a rule-based system to generate a snapshot of the post. Ex:\n",
    "- score each comment: score = upvotes + replys * 10\n",
    "- output the top 10 comments + any parents\n",
    "- generate the yaml to use as input for the llm\n",
    "\n",
    "```yaml\n",
    "post_title: \"Why are MCPs needed for basic tools like filesystem access, git etc?\"\n",
    "post_body: \"Im puzzled why an MCP (that too written in TS) is required for something as basic as reading, listing files etc. In my experience, the LLM has no problem in doing these tasks without any reliability concerns without any MCP. Likewise for git and even gh.\"\n",
    "children:\n",
    "    - comment: \"MCP servers aren't NEEDED for that, but by using it, then you can re-use that functionality across clients vs implementing it each time in the client. That being said, the embedded functionality would likely work faster/better because of the tighter integration. So basically, you pick your sensitivity to effort cost and take the appropriate route.\"\n",
    "    - comment: \"LLMs can generate commands but can't actually execute them locally. They have no access to your local resources (e.g. files, apps). Simply put, MCP bridges that gap so the LLM can actually interact with your real environment instead of you having to copy paste commands back and forth. Additionally, they serve as the building blocks for agentic workflows.\"\n",
    "      children:\n",
    "      - comment: \"this is the right answer, truly. u/rm-rf-rm your fundamental assumption is incorrect - \\\"LLM ha sno problem in doing these\\\" is incorrect. LLMs can say \\\"list directories in ...\\\" but it then has to be a tool call by the MCP client that performs that. the question maybe then becomes - should capabilities like file system be implemented by MCP clients by default or be provided via MCPs.\"\n",
    "      - comment: \"u/rm-rf-rm might have eval {llm_response} where llm_respose is something like git log. In that case s/he might be correct that llm + sh love can do that\"\n",
    "    - comment: \"Sure but I think the bigger question is why a separate MCP for Git, LS etc. and not just give the ability to run Bash commands\"\n",
    "      children:\n",
    "        - comment: \"One reason is that the MCP server is self-contained, therefore decoupled from the user's environment and OS specifics. As an example, a git command execution via shell would fail if git is not installed or is not in the path. Another is that the MCP layer allows the LLM to operate at the git/version abstraction level not only the shell invocation one, allowing for more precision in terms of selecting the correct tool and its proper usage.\"\n",
    "    - comment: \"Maybe you are referring to using an AI powered coding tool? Those have AI tools built for reading files, accessing the shell, etc. the power of MCP is you don't need to build a tool that is restricted to a specific AI coding tool, you can just expose the MCP server remotely and any LLM that supports MCP can access it.\"\n",
    "    - comment: \"They're not needed; most clients have built-in tools for filesystem access. For git, you can just tell the agent to use the \\\"gh\\\" command. Same for anything where a command exists.\"\n",
    "      children:\n",
    "        - comment: \"For git, you can just tell the agent to use the \\\"gh\\\" command gh does not perform most git actions (add, commit, push, etc.) - instead, it is for github-specific actions which aren't normally handled by the 'git' CLI app. Same for anything where a command exists. Agents may be able to use CLI commands such as git, yes, but they work better with the git MCP etc. most clients have built-in tools for filesystem access. You may be correct about this though.\"\n",
    "```\n",
    "\n",
    "## Step 3: Query the LLM:\n",
    "```yaml\n",
    "prompt: \"generate a short linkedin post from this conversation. Extract the debate or lightbulb moment from this thread and present it.\"\n",
    "context: {article_and_comments}\n",
    "```\n",
    "\n",
    "## Step 4: Render the Output\n",
    "\n",
    "**ðŸ”§ The Great MCP Debate: Building Blocks vs. Built-ins**\n",
    "\n",
    "Had a fascinating discussion about why we need Model Context Protocol (MCP) servers for \"basic\" tasks like file operations and git commands. The lightbulb moment? It's not about what LLMs *can* generateâ€”it's about execution and reusability.\n",
    "\n",
    "**The core insight:** LLMs can write perfect bash commands, but they can't actually *run* them. That's where MCPs bridge the gap between AI suggestions and real system interaction.\n",
    "\n",
    "**But here's the real debate:** Should we have specialized MCPs for git, filesystem, etc., or just give LLMs direct shell access?\n",
    "\n",
    "**The case for specialized MCPs:**\n",
    "â€¢ Abstraction over raw commands (git operations vs. shell invocations)  \n",
    "â€¢ Environment-agnostic (works regardless of OS or installed tools)\n",
    "â€¢ Reusable across different AI clients\n",
    "â€¢ Better error handling and context\n",
    "\n",
    "**The case for shell access:**\n",
    "â€¢ Simpler implementation\n",
    "â€¢ Leverages existing tooling\n",
    "â€¢ More flexible for edge cases\n",
    "\n",
    "The thread revealed a fundamental tension in AI tooling: granular, safe abstractions vs. powerful, direct access. Both approaches have merit depending on your use case and risk tolerance.\n",
    "\n",
    "What's your take? Specialized tools or universal shell access for AI agents?\n",
    "\n",
    "#AI #MCP #DeveloperTools #LLM #Automation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00af0e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<praw.reddit.Reddit object at 0xff6e9747b410>\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "REDDIT_CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "REDDIT_CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "REDDIT_USERNAME = os.getenv(\"REDDIT_USERNAME\")\n",
    "REDDIT_PASSWORD = os.getenv(\"REDDIT_PASSWORD\")\n",
    "\n",
    "# Authenticate with reddit api\n",
    "reddit = praw.Reddit(\n",
    "    client_id = REDDIT_CLIENT_ID,\n",
    "    client_secret = REDDIT_CLIENT_SECRET,\n",
    "    user_agent = REDDIT_USERNAME\n",
    ")\n",
    "\n",
    "print(reddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f114b4db",
   "metadata": {},
   "source": [
    "## Retrieve a Random Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "347d888b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google just dropped an ace 64-page guide on building AI Agents\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "\n",
    "def select_random_submission(subreddits ,limit, min_comments):\n",
    "    # Are there any exceptions we should be catching here?\n",
    "    sub = subreddit_list[randrange(0,len(subreddits))]\n",
    "    submissions_list = []\n",
    "    \n",
    "    \n",
    "    for submission in reddit.subreddit(sub).hot(limit=limit):\n",
    "        if submission.num_comments > min_comments:\n",
    "            submissions_list.append({\n",
    "                \"subreddit\": sub,\n",
    "                \"title\": submission.title,\n",
    "                \"score\": submission.score,\n",
    "                \"id\": submission.id,\n",
    "                \"comments\": submission.num_comments\n",
    "            })\n",
    "        \n",
    "    if len(submissions_list) > 0:\n",
    "        return submissions_list[randrange(0, len(submissions_list))]\n",
    "    else:\n",
    "        raise Exception(f\"No submissions found in {sub} matching filter\")\n",
    "\n",
    "# Select a random sub and retrieve a list of hot posts.\n",
    "subreddit_list = ['mcp', 'vibecoding', 'buildinpublic', 'aws', 'LlamaFarm', 'AgentsOfAI', 'ClaudeAI', 'Buildathon']\n",
    "random_submission = select_random_submission(subreddit_list, 10, 10)\n",
    "print(random_submission[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b05c901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'nfm6ay2', 'author': 'Hans_lilly_Gruber', 'body': \"Careful, clicking the link automatically launches a download of a file and there's no way of seeing what.\", 'score': 3, 'children': ['nfmdkmj', 'nfmuwv6', 'nfqgdzz']}, {'id': 'nfmdkmj', 'author': 'SlyThought', 'body': 'If you open it on Brave, it opens as a PDF without needing to download it(as it does for pdf files in general).', 'score': 2, 'children': ['nfmuvel']}, {'id': 'nfmuvel', 'author': 'Valuable_Simple3860', 'body': \"Yes. Pls open in browser. It's a pdf. No need to download.\", 'score': 2, 'children': ['nfqgj0j', 'nfqun3s']}, {'id': 'nfqgj0j', 'author': 'littlemetal', 'body': 'How does the browser display it without downloading it?', 'score': 1, 'children': []}, {'id': 'nfqun3s', 'author': 'exiadf19', 'body': 'Bruh, when you open pdf in browser, it already download to your pc / mobile phone. Unless you have an embedded system that using a viewer', 'score': 1, 'children': []}, {'id': 'nfmuwv6', 'author': 'Valuable_Simple3860', 'body': 'Open in browser.', 'score': 2, 'children': []}, {'id': 'nfqgdzz', 'author': 'littlemetal', 'body': 'No way...  Hast thee not eyes to see, and fingers to clicketh the mouse and drill unto the folders?  \\n\\nLinks to pdfs on google.com must be treated with the utmost suspicion.  Only if they are \"downloaded\" into the cache directory and displayed by they browser shall they be considered safe.', 'score': 1, 'children': []}, {'id': 'nfmpb01', 'author': 'runningblind77', 'body': '[Direct Link](https://services.google.com/fh/files/misc/startup_technical_guide_ai_agents_final.pdf)\\n\\nChecksums don\\'t match between the official direct link and the \"Source\" above, but the \"Source\" is substantially smaller than the official link for some reason.', 'score': 1, 'children': ['nfqgmvg']}, {'id': 'nfqgmvg', 'author': 'littlemetal', 'body': 'Has the link been edited?  Yours and that are identical, bit for bit.', 'score': 1, 'children': ['nfs2rzc']}, {'id': 'nfs2rzc', 'author': 'runningblind77', 'body': 'Yeah, yesterday the link pointed to [media.licdn.com](https://media.licdn.com/dms/document/media/v2/D4D1FAQFqManoGTtsmQ/feedshare-document-pdf-analyzed/B4DZlsAU1FGkAY-/0/1758453662268?e=1759363200&v=beta&t=JLse1O-hbDMYQ_UN0Gi-u43fI7MB-KoG4cupQhuVf5Q). It was edited a little over an hour after my comment.', 'score': 1, 'children': []}, {'id': 'ng0f0m2', 'author': 'ItzDaReaper', 'body': 'Why donâ€™t any of the links workâ€¦', 'score': 1, 'children': []}]\n"
     ]
    }
   ],
   "source": [
    "def extract_comment_recursively(comment):\n",
    "    \"\"\"Recursively extract comment data including nested replies\"\"\"\n",
    "    if not hasattr(comment, 'body'):  # Skip non-comment objects\n",
    "        return None\n",
    "    \n",
    "    # Get children IDs recursively\n",
    "    children_ids = []\n",
    "    if hasattr(comment, 'replies') and comment.replies:\n",
    "        for reply in comment.replies:\n",
    "            if hasattr(reply, 'body'):  # Make sure it's a comment\n",
    "                children_ids.append(reply.id)\n",
    "    \n",
    "    return {\n",
    "        \"id\": comment.id,\n",
    "        \"author\": comment.author.name if comment.author else '[deleted]',\n",
    "        \"body\": comment.body,\n",
    "        \"score\": comment.score,\n",
    "        \"children\": children_ids\n",
    "    }\n",
    "\n",
    "def extract_all_comments_recursively(comment, all_comments):\n",
    "    \"\"\"Recursively extract all comments and their nested replies\"\"\"\n",
    "    if not hasattr(comment, 'body'):  # Skip non-comment objects\n",
    "        return\n",
    "    \n",
    "    # Extract current comment\n",
    "    comment_data = extract_comment_recursively(comment)\n",
    "    if comment_data:\n",
    "        all_comments.append(comment_data)\n",
    "    \n",
    "    # Recursively process replies\n",
    "    if hasattr(comment, 'replies') and comment.replies:\n",
    "        for reply in comment.replies:\n",
    "            extract_all_comments_recursively(reply, all_comments)\n",
    "\n",
    "\n",
    "# get the example post\n",
    "submission = reddit.submission(id=random_submission['id'])\n",
    "# Extract all comments recursively\n",
    "comments_list = []\n",
    "submission.comments.replace_more(limit=0)  # Remove \"more comments\" objects\n",
    "for comment in submission.comments:\n",
    "    extract_all_comments_recursively(comment, comments_list)\n",
    "\n",
    "print(comments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79e6688c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated YAML content:\n",
      "==================================================\n",
      "post_title: Google just dropped an ace 64-page guide on building AI Agents\n",
      "post_body: '[Source](https://services.google.com/fh/files/misc/startup_technical_guide_ai_agents_final.pdf)'\n",
      "children:\n",
      "- comment: Careful, clicking the link automatically launches a download of a file\n",
      "    and there's no way of seeing what.\n",
      "- comment: '[Direct Link](https://services.google.com/fh/files/misc/startup_technical_guide_ai_agents_final.pdf)\n",
      "\n",
      "\n",
      "    Checksums don''t match between the official direct link and the \"Source\" above,\n",
      "    but the \"Source\" is substantially smaller than the official link for some reason.'\n",
      "\n",
      "\n",
      "Querying Gemini API...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758907841.134340  282749 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini Response:\n",
      "Here are a few options for a short LinkedIn post, each with a slightly different focus.\n",
      "\n",
      "### Option 1 (Focus on Community & Security)\n",
      "\n",
      "**Headline:** A great resource, and an even better lesson in digital trust.\n",
      "\n",
      "Google just dropped a fantastic 64-page guide on building AI Agents, but the real lightbulb moment came from the community's response to a shared link.\n",
      "\n",
      "The initial link being passed around wasn't the official oneâ€”it auto-downloaded a smaller, unverified file. Community members quickly flagged it, compared checksums, and provided the direct, official source.\n",
      "\n",
      "Itâ€™s a powerful reminder: Always verify your sources, even when the content seems legitimate. A quick check is your first line of defense.\n",
      "\n",
      "Kudos to the vigilant tech community for keeping everyone safe.\n",
      "\n",
      "Here's the verified guide: [https://services.google.com/fh/files/misc/startup_technical_guide_ai_agents_final.pdf](https://services.google.com/fh/files/misc/startup_technical_guide_ai_agents_final.pdf)\n",
      "\n",
      "#AI #Cybersecurity #TechCommunity #TrustButVerify #AIAgents\n",
      "\n",
      "---\n",
      "\n",
      "### Option 2 (More Direct & Action-Oriented)\n",
      "\n",
      "**Headline:** Before you download that new Google AI guide...\n",
      "\n",
      "There's an excellent new 64-page guide from Google on building AI Agents making the rounds. But a discussion I saw highlighted a critical point: not all links are created equal.\n",
      "\n",
      "The original link shared in one forum forced an automatic download of a file that was different from the official version.\n",
      "\n",
      "The takeaway? Even for resources from trusted names like Google, always double-check that the link is from the official source.\n",
      "\n",
      "Sharing the direct, verified link below for anyone building in the AI space.\n",
      "\n",
      "Get the guide here: [https://services.google.com/fh/files/misc/startup_technical_guide_ai_agents_final.pdf](https://services.google.com/fh/files/misc/startup_technical_guide_ai_agents_final.pdf)\n",
      "\n",
      "#AI #Google #Developer #MachineLearning #DigitalLiteracy\n",
      "\n",
      "---\n",
      "\n",
      "### Option 3 (Short & Punchy)\n",
      "\n",
      "**Headline:** That new Google AI guide? Make sure you have the right one.\n",
      "\n",
      "A conversation about Google's new 64-page AI Agent guide sparked an important debate on link integrity.\n",
      "\n",
      "The lesson: An unofficial link was circulating that was smaller and unverified compared to the real deal.\n",
      "\n",
      "It's a simple but crucial reminder to always vet your sources. The community's quick catch was a perfect example of why that matters.\n",
      "\n",
      "Here is the official, safe link: [https://services.google.com/fh/files/misc/startup_technical_guide_ai_agents_final.pdf](https://services.google.com/fh/files/misc/startup_technical_guide_ai_agents_final.pdf)\n",
      "\n",
      "#AIAgents #Tech #CybersecurityAwareness #Community\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "def generate_llm_content(submission, comments_list, top_n_comments: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Generate LLM-ready content by extracting top N comments and building a hierarchical structure\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort comments by score and get top N\n",
    "    sorted_comments = sorted(comments_list, key=lambda x: x['score'], reverse=True)[:top_n_comments]\n",
    "    \n",
    "    # Create a lookup dictionary for all comments\n",
    "    comment_lookup = {comment['id']: comment for comment in comments_list}\n",
    "    \n",
    "    def build_comment_structure(comment_data: dict) -> dict:\n",
    "        \"\"\"Build a comment structure with its children\"\"\"\n",
    "        structure = {\n",
    "            \"comment\": comment_data['body']\n",
    "        }\n",
    "        \n",
    "        # Get children comments if they exist\n",
    "        if comment_data['children']:\n",
    "            # Build children structures recursively\n",
    "            children_structures = []\n",
    "            for child_id in comment_data['children']:\n",
    "                if child_id in comment_lookup:\n",
    "                    child_structure = build_comment_structure(comment_lookup[child_id])\n",
    "                    children_structures.append(child_structure)\n",
    "            \n",
    "        \n",
    "        return structure\n",
    "    \n",
    "    # Build the main structure\n",
    "    llm_structure = {\n",
    "        \"post_title\": submission.title,\n",
    "        \"post_body\": submission.selftext,\n",
    "        \"children\": []\n",
    "    }\n",
    "    \n",
    "    # Find top-level comments (those that are in our top N and don't appear as children of others)\n",
    "    all_child_ids = set()\n",
    "    for comment in comments_list:\n",
    "        all_child_ids.update(comment['children'])\n",
    "    \n",
    "    top_level_comments = [c for c in sorted_comments if c['id'] not in all_child_ids]\n",
    "    \n",
    "    # Build comment structures for top-level comments\n",
    "    for comment in top_level_comments:\n",
    "        comment_structure = build_comment_structure(comment)\n",
    "        llm_structure[\"children\"].append(comment_structure)\n",
    "    \n",
    "    # Convert to YAML and return\n",
    "    return yaml.dump(llm_structure, default_flow_style=False, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "def query_gemini_with_content(yaml_content: str, prompt: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Query Google Gemini API with the YAML content using LangChain\n",
    "    \"\"\"\n",
    "    # Get API key from environment\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"GEMINI_API_KEY environment variable not found\")\n",
    "    \n",
    "    # Initialize the Gemini model\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-pro\",\n",
    "        google_api_key=api_key,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    # Default prompt if none provided\n",
    "    if not prompt:\n",
    "        prompt = \"Generate a short LinkedIn post from this conversation. Extract the debate or lightbulb moment from this thread and present it.\"\n",
    "    \n",
    "    # Combine prompt with YAML content\n",
    "    full_prompt = f\"{prompt}\\n\\nContext:\\n{yaml_content}\"\n",
    "    \n",
    "    # Query the model\n",
    "    response = llm.invoke(full_prompt)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "# Generate the LLM content using the new structure\n",
    "llm_content = generate_llm_content(submission, comments_list, top_n_comments=10)\n",
    "print(\"Generated YAML content:\")\n",
    "print(\"=\" * 50)\n",
    "print(llm_content)\n",
    "\n",
    "# Query Gemini with the content\n",
    "print(\"\\nQuerying Gemini API...\")\n",
    "print(\"=\" * 50)\n",
    "try:\n",
    "    gemini_response = query_gemini_with_content(\n",
    "        llm_content, \n",
    "        \"Generate a short LinkedIn post from this conversation. Extract the debate or lightbulb moment from this thread and present it.\"\n",
    "    )\n",
    "    print(\"Gemini Response:\")\n",
    "    print(gemini_response)\n",
    "except Exception as e:\n",
    "    print(f\"Error querying Gemini: {e}\")\n",
    "    print(\"Make sure GEMINI_API_KEY is set in your environment and langchain-google-genai is installed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse7319-term-project (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
