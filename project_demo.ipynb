{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffdcde6b",
   "metadata": {},
   "source": [
    "Based on the following video: [How-to Use The Reddit API in Python by James Briggs](https://www.youtube.com/watch?v=FdjVoOf9HN4)\n",
    "1. Create a reddit application [here](https://www.reddit.com/prefs/apps) to get a client_id and client_secret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b8e12",
   "metadata": {},
   "source": [
    "# Example workflow\n",
    "\n",
    "## Step 1: Find a Trending Post\n",
    "Use various sorting techniques such as hot, trending, controversial to select a post with insightful information. [Example post](https://www.reddit.com/r/mcp/comments/1nculrw/why_are_mcps_needed_for_basic_tools_like/)\n",
    "\n",
    "## Step 2: Convert the Post Into a YAML Summary\n",
    "Apply a rule-based system to generate a snapshot of the post. Ex:\n",
    "- score each comment: score = upvotes + replys * 10\n",
    "- output the top 10 comments + any parents\n",
    "- generate the yaml to use as input for the llm\n",
    "\n",
    "```yaml\n",
    "post_title: \"Why are MCPs needed for basic tools like filesystem access, git etc?\"\n",
    "post_body: \"Im puzzled why an MCP (that too written in TS) is required for something as basic as reading, listing files etc. In my experience, the LLM has no problem in doing these tasks without any reliability concerns without any MCP. Likewise for git and even gh.\"\n",
    "children:\n",
    "    - comment: \"MCP servers aren't NEEDED for that, but by using it, then you can re-use that functionality across clients vs implementing it each time in the client. That being said, the embedded functionality would likely work faster/better because of the tighter integration. So basically, you pick your sensitivity to effort cost and take the appropriate route.\"\n",
    "    - comment: \"LLMs can generate commands but can't actually execute them locally. They have no access to your local resources (e.g. files, apps). Simply put, MCP bridges that gap so the LLM can actually interact with your real environment instead of you having to copy paste commands back and forth. Additionally, they serve as the building blocks for agentic workflows.\"\n",
    "      children:\n",
    "      - comment: \"this is the right answer, truly. u/rm-rf-rm your fundamental assumption is incorrect - \\\"LLM ha sno problem in doing these\\\" is incorrect. LLMs can say \\\"list directories in ...\\\" but it then has to be a tool call by the MCP client that performs that. the question maybe then becomes - should capabilities like file system be implemented by MCP clients by default or be provided via MCPs.\"\n",
    "      - comment: \"u/rm-rf-rm might have eval {llm_response} where llm_respose is something like git log. In that case s/he might be correct that llm + sh love can do that\"\n",
    "    - comment: \"Sure but I think the bigger question is why a separate MCP for Git, LS etc. and not just give the ability to run Bash commands\"\n",
    "      children:\n",
    "        - comment: \"One reason is that the MCP server is self-contained, therefore decoupled from the user's environment and OS specifics. As an example, a git command execution via shell would fail if git is not installed or is not in the path. Another is that the MCP layer allows the LLM to operate at the git/version abstraction level not only the shell invocation one, allowing for more precision in terms of selecting the correct tool and its proper usage.\"\n",
    "    - comment: \"Maybe you are referring to using an AI powered coding tool? Those have AI tools built for reading files, accessing the shell, etc. the power of MCP is you don't need to build a tool that is restricted to a specific AI coding tool, you can just expose the MCP server remotely and any LLM that supports MCP can access it.\"\n",
    "    - comment: \"They're not needed; most clients have built-in tools for filesystem access. For git, you can just tell the agent to use the \\\"gh\\\" command. Same for anything where a command exists.\"\n",
    "      children:\n",
    "        - comment: \"For git, you can just tell the agent to use the \\\"gh\\\" command gh does not perform most git actions (add, commit, push, etc.) - instead, it is for github-specific actions which aren't normally handled by the 'git' CLI app. Same for anything where a command exists. Agents may be able to use CLI commands such as git, yes, but they work better with the git MCP etc. most clients have built-in tools for filesystem access. You may be correct about this though.\"\n",
    "```\n",
    "\n",
    "## Step 3: Query the LLM:\n",
    "```yaml\n",
    "prompt: \"generate a short linkedin post from this conversation. Extract the debate or lightbulb moment from this thread and present it.\"\n",
    "context: {article_and_comments}\n",
    "```\n",
    "\n",
    "## Step 4: Render the Output\n",
    "\n",
    "**ðŸ”§ The Great MCP Debate: Building Blocks vs. Built-ins**\n",
    "\n",
    "Had a fascinating discussion about why we need Model Context Protocol (MCP) servers for \"basic\" tasks like file operations and git commands. The lightbulb moment? It's not about what LLMs *can* generateâ€”it's about execution and reusability.\n",
    "\n",
    "**The core insight:** LLMs can write perfect bash commands, but they can't actually *run* them. That's where MCPs bridge the gap between AI suggestions and real system interaction.\n",
    "\n",
    "**But here's the real debate:** Should we have specialized MCPs for git, filesystem, etc., or just give LLMs direct shell access?\n",
    "\n",
    "**The case for specialized MCPs:**\n",
    "â€¢ Abstraction over raw commands (git operations vs. shell invocations)  \n",
    "â€¢ Environment-agnostic (works regardless of OS or installed tools)\n",
    "â€¢ Reusable across different AI clients\n",
    "â€¢ Better error handling and context\n",
    "\n",
    "**The case for shell access:**\n",
    "â€¢ Simpler implementation\n",
    "â€¢ Leverages existing tooling\n",
    "â€¢ More flexible for edge cases\n",
    "\n",
    "The thread revealed a fundamental tension in AI tooling: granular, safe abstractions vs. powerful, direct access. Both approaches have merit depending on your use case and risk tolerance.\n",
    "\n",
    "What's your take? Specialized tools or universal shell access for AI agents?\n",
    "\n",
    "#AI #MCP #DeveloperTools #LLM #Automation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00af0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "REDDIT_CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "REDDIT_CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "REDDIT_USERNAME = os.getenv(\"REDDIT_USERNAME\")\n",
    "REDDIT_PASSWORD = os.getenv(\"REDDIT_PASSWORD\")\n",
    "\n",
    "# Authenticate with reddit api\n",
    "reddit = praw.Reddit(\n",
    "    client_id = REDDIT_CLIENT_ID,\n",
    "    client_secret = REDDIT_CLIENT_SECRET,\n",
    "    user_agent = REDDIT_USERNAME\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab5588b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the example post\n",
    "submission = reddit.submission(id=\"1nculrw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d5d1df",
   "metadata": {},
   "outputs": [],
   "source": "import json\n\ndef extract_comment_tree(comment, max_depth=3, current_depth=0):\n    \"\"\"Recursively extract comment data into a tree structure\"\"\"\n    if current_depth >= max_depth:\n        return None\n    \n    comment_data = {\n        'id': comment.id,\n        'author': str(comment.author) if comment.author else '[deleted]',\n        'body': comment.body,\n        'score': comment.score,\n        'created_utc': comment.created_utc,\n        'replies': []\n    }\n    \n    # Process replies if they exist and we haven't reached max depth\n    if hasattr(comment, 'replies') and comment.replies and current_depth < max_depth - 1:\n        for reply in comment.replies:\n            if hasattr(reply, 'body'):  # Make sure it's actually a comment\n                reply_data = extract_comment_tree(reply, max_depth, current_depth + 1)\n                if reply_data:\n                    comment_data['replies'].append(reply_data)\n    \n    return comment_data\n\n# Extract post data\npost_data = {\n    'title': submission.title,\n    'selftext': submission.selftext,\n    'score': submission.score,\n    'upvote_ratio': submission.upvote_ratio,\n    'num_comments': submission.num_comments,\n    'created_utc': submission.created_utc,\n    'url': submission.url,\n    'id': submission.id,\n    'comments': []\n}\n\n# Process top-level comments\nsubmission.comments.replace_more(limit=0)  # Remove \"more comments\" objects\nfor comment in submission.comments:\n    if hasattr(comment, 'body'):  # Make sure it's actually a comment\n        comment_tree = extract_comment_tree(comment, max_depth=3)\n        if comment_tree:\n            post_data['comments'].append(comment_tree)\n\n# Print the JSON structure\nprint(json.dumps(post_data, indent=2, default=str))"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse7319-term-project (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}