{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffdcde6b",
   "metadata": {},
   "source": [
    "Based on the following video: [How-to Use The Reddit API in Python by James Briggs](https://www.youtube.com/watch?v=FdjVoOf9HN4)\n",
    "1. Create a reddit application [here](https://www.reddit.com/prefs/apps) to get a client_id and client_secret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b8e12",
   "metadata": {},
   "source": [
    "# Example workflow\n",
    "\n",
    "## Step 1: Find a Trending Post\n",
    "Use various sorting techniques such as hot, trending, controversial to select a post with insightful information. [Example post](https://www.reddit.com/r/mcp/comments/1nculrw/why_are_mcps_needed_for_basic_tools_like/)\n",
    "\n",
    "## Step 2: Convert the Post Into a YAML Summary\n",
    "Apply a rule-based system to generate a snapshot of the post. Ex:\n",
    "- score each comment: score = upvotes + replys * 10\n",
    "- output the top 10 comments + any parents\n",
    "- generate the yaml to use as input for the llm\n",
    "\n",
    "```yaml\n",
    "post_title: \"Why are MCPs needed for basic tools like filesystem access, git etc?\"\n",
    "post_body: \"Im puzzled why an MCP (that too written in TS) is required for something as basic as reading, listing files etc. In my experience, the LLM has no problem in doing these tasks without any reliability concerns without any MCP. Likewise for git and even gh.\"\n",
    "children:\n",
    "    - comment: \"MCP servers aren't NEEDED for that, but by using it, then you can re-use that functionality across clients vs implementing it each time in the client. That being said, the embedded functionality would likely work faster/better because of the tighter integration. So basically, you pick your sensitivity to effort cost and take the appropriate route.\"\n",
    "    - comment: \"LLMs can generate commands but can't actually execute them locally. They have no access to your local resources (e.g. files, apps). Simply put, MCP bridges that gap so the LLM can actually interact with your real environment instead of you having to copy paste commands back and forth. Additionally, they serve as the building blocks for agentic workflows.\"\n",
    "      children:\n",
    "      - comment: \"this is the right answer, truly. u/rm-rf-rm your fundamental assumption is incorrect - \\\"LLM ha sno problem in doing these\\\" is incorrect. LLMs can say \\\"list directories in ...\\\" but it then has to be a tool call by the MCP client that performs that. the question maybe then becomes - should capabilities like file system be implemented by MCP clients by default or be provided via MCPs.\"\n",
    "      - comment: \"u/rm-rf-rm might have eval {llm_response} where llm_respose is something like git log. In that case s/he might be correct that llm + sh love can do that\"\n",
    "    - comment: \"Sure but I think the bigger question is why a separate MCP for Git, LS etc. and not just give the ability to run Bash commands\"\n",
    "      children:\n",
    "        - comment: \"One reason is that the MCP server is self-contained, therefore decoupled from the user's environment and OS specifics. As an example, a git command execution via shell would fail if git is not installed or is not in the path. Another is that the MCP layer allows the LLM to operate at the git/version abstraction level not only the shell invocation one, allowing for more precision in terms of selecting the correct tool and its proper usage.\"\n",
    "    - comment: \"Maybe you are referring to using an AI powered coding tool? Those have AI tools built for reading files, accessing the shell, etc. the power of MCP is you don't need to build a tool that is restricted to a specific AI coding tool, you can just expose the MCP server remotely and any LLM that supports MCP can access it.\"\n",
    "    - comment: \"They're not needed; most clients have built-in tools for filesystem access. For git, you can just tell the agent to use the \\\"gh\\\" command. Same for anything where a command exists.\"\n",
    "      children:\n",
    "        - comment: \"For git, you can just tell the agent to use the \\\"gh\\\" command gh does not perform most git actions (add, commit, push, etc.) - instead, it is for github-specific actions which aren't normally handled by the 'git' CLI app. Same for anything where a command exists. Agents may be able to use CLI commands such as git, yes, but they work better with the git MCP etc. most clients have built-in tools for filesystem access. You may be correct about this though.\"\n",
    "```\n",
    "\n",
    "## Step 3: Query the LLM:\n",
    "```yaml\n",
    "prompt: \"generate a short linkedin post from this conversation. Extract the debate or lightbulb moment from this thread and present it.\"\n",
    "context: {article_and_comments}\n",
    "```\n",
    "\n",
    "## Step 4: Render the Output\n",
    "\n",
    "**ðŸ”§ The Great MCP Debate: Building Blocks vs. Built-ins**\n",
    "\n",
    "Had a fascinating discussion about why we need Model Context Protocol (MCP) servers for \"basic\" tasks like file operations and git commands. The lightbulb moment? It's not about what LLMs *can* generateâ€”it's about execution and reusability.\n",
    "\n",
    "**The core insight:** LLMs can write perfect bash commands, but they can't actually *run* them. That's where MCPs bridge the gap between AI suggestions and real system interaction.\n",
    "\n",
    "**But here's the real debate:** Should we have specialized MCPs for git, filesystem, etc., or just give LLMs direct shell access?\n",
    "\n",
    "**The case for specialized MCPs:**\n",
    "â€¢ Abstraction over raw commands (git operations vs. shell invocations)  \n",
    "â€¢ Environment-agnostic (works regardless of OS or installed tools)\n",
    "â€¢ Reusable across different AI clients\n",
    "â€¢ Better error handling and context\n",
    "\n",
    "**The case for shell access:**\n",
    "â€¢ Simpler implementation\n",
    "â€¢ Leverages existing tooling\n",
    "â€¢ More flexible for edge cases\n",
    "\n",
    "The thread revealed a fundamental tension in AI tooling: granular, safe abstractions vs. powerful, direct access. Both approaches have merit depending on your use case and risk tolerance.\n",
    "\n",
    "What's your take? Specialized tools or universal shell access for AI agents?\n",
    "\n",
    "#AI #MCP #DeveloperTools #LLM #Automation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00af0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "REDDIT_CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "REDDIT_CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "REDDIT_USERNAME = os.getenv(\"REDDIT_USERNAME\")\n",
    "REDDIT_PASSWORD = os.getenv(\"REDDIT_PASSWORD\")\n",
    "\n",
    "# Authenticate with reddit api\n",
    "reddit = praw.Reddit(\n",
    "    client_id = REDDIT_CLIENT_ID,\n",
    "    client_secret = REDDIT_CLIENT_SECRET,\n",
    "    user_agent = REDDIT_USERNAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab5588b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the example post\n",
    "submission = reddit.submission(id=\"1nculrw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20d5d1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"title\": \"Why are MCPs needed for basic tools like filesystem access, git etc?\",\n",
      "    \"selftext\": \"Im puzzled why an MCP (that too written in TS) is required for something as basic as reading, listing files etc. In my experience, the LLM has no problem in doing these tasks without any reliability concerns without any MCP. Likewise for git and even gh. \",\n",
      "    \"score\": 3,\n",
      "    \"url\": \"https://www.reddit.com/r/mcp/comments/1nculrw/why_are_mcps_needed_for_basic_tools_like/\",\n",
      "    \"id\": \"1nculrw\",\n",
      "    \"comments\": [\n",
      "        {\n",
      "            \"body\": \"MCP servers aren't NEEDED for that, but by using it, then you can re-use that functionality across clients vs implementing it each time in the client.    That being said, the embedded functionality would likely work faster/better because of the tighter integration.\\n\\nSo basically, you pick your sensitivity to effort cost and take the appropriate route.\",\n",
      "            \"score\": 9,\n",
      "            \"replies\": []\n",
      "        },\n",
      "        {\n",
      "            \"body\": \"LLMs can generate commands but can't actually execute them locally. They have no access to your local resources (e.g. files, apps). Simply put, MCP bridges that gap so the LLM can actually interact with your real environment instead of you having to copy paste commands back and forth. Additionally, they serve as the building blocks for agentic workflows.\",\n",
      "            \"score\": 12,\n",
      "            \"replies\": [\n",
      "                {\n",
      "                    \"body\": \"this is the right answer, truly. u/rm-rf-rm your fundamental assumption is incorrect - \\\"LLM ha sno problem in doing these\\\" is incorrect. LLMs can say \\\"list directories in ...\\\" but it then has to be a tool call by the MCP client that performs that.\\n\\nthe question maybe then becomes - should capabilities like file system be implemented by MCP clients by default or be provided via MCPs.\",\n",
      "                    \"score\": 8,\n",
      "                    \"replies\": [\n",
      "                        {\n",
      "                            \"body\": \"u/rm-rf-rm might have `eval {llm_response}` where llm_respose is something like `git log`. In that case s/he might be correct that llm + sh love can do that\",\n",
      "                            \"score\": 5,\n",
      "                            \"replies\": [\n",
      "                                {\n",
      "                                    \"body\": \"You either don't understand the role of LLM and a client or I'm missing something. How would the shell evaluation works? who spins up a new process? certainly not the LLM. the LLM is literally, by definition, a model. The model can't execute anything, not shell and not function calls.\",\n",
      "                                    \"score\": 4,\n",
      "                                    \"replies\": [\n",
      "                                        {\n",
      "                                            \"body\": \"ðŸ¤— Oh! I meant  llm + shell love, meaning more than just LLM.  For example\\n\\n\\n```sh\\n# set llm cli\\nexecute llm --system 'Reply with linux terminal commands only, no extra information' --save cmd &>/dev/null\\n\\n# .aliases => .bash_alias\\nalias cmd='llm_func() { command llm -t cmd \\\"$@\\\" | xargs -I {} sh -c \\\"echo \\\\\\\"execute:\\\\n\\\\t {}\\\\n\\\\\\\"; eval {}\\\"; }; llm_func'\\n\\n```\\n\\nWith that cmd \\\"show me last commits\\\"\",\n",
      "                                            \"score\": 1,\n",
      "                                            \"replies\": []\n",
      "                                        }\n",
      "                                    ]\n",
      "                                }\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"body\": \"Arguably you could equip an LLM with a single tool \\\"bash\\\" and it would be all it needs. MCP just makes it more likely that it will succeed.\",\n",
      "                            \"score\": 4,\n",
      "                            \"replies\": [\n",
      "                                {\n",
      "                                    \"body\": \"The point isn't what you equip the LLM with it's that fundamentally, an LLM can't invoke anything, it can only generate text that would be interpreted by a client which mitigates the calling.\",\n",
      "                                    \"score\": 5,\n",
      "                                    \"replies\": [\n",
      "                                        {\n",
      "                                            \"body\": \"Yes. Of course. But if you build one single tool integration, bash, then in principle you donâ€™t really need MCP. Tool calling via MCP reduces the error rate by catering to the limitations of todayâ€™s LLMs.\",\n",
      "                                            \"score\": 4,\n",
      "                                            \"replies\": [\n",
      "                                                {\n",
      "                                                    \"body\": \"You're replying on a totally different topic :-)\\n\\nI addressed the core issue that was said in the OP message:  \\n\\\\> the LLM has no problem in doing these tasks without any reliability concerns without any MCP\\n\\nFundamentally, the LLM isn't doing any tasks, not function calls, not tool calls and not MCP calls. Maybe the OP learns from this. Nuance is important.\",\n",
      "                                                    \"score\": 5,\n",
      "                                                    \"replies\": [\n",
      "                                                        {\n",
      "                                                            \"body\": \"Fundamentally, the LLM doesn't produce anything other than logits. You can add as much scaffolding on top of that as you want, like calculating the probability density over the token vocabulary, or picking and producing token strings, or parsing the strings for turn taking, or you can go further and parse for tool calling, or even execute the tools, or go all in with a suit of MCP servers, and so on. I feel you are arbitrarily drawing the line of where the LLM starts and stops.\\n\\nTo me, I would put the fundamental boundary of an LLM to be when it produces probability density vectors over the token vocabulary. This is probably because of my background in ML. When it produces literal text, and starts taking turns in a chat session, that is what OpenAI would call an Assistant, and when you throw in tool calling in the mix, I would call that an Agent.\",\n",
      "                                                            \"score\": 1,\n",
      "                                                            \"replies\": []\n",
      "                                                        }\n",
      "                                                    ]\n",
      "                                                }\n",
      "                                            ]\n",
      "                                        }\n",
      "                                    ]\n",
      "                                }\n",
      "                            ]\n",
      "                        }\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"body\": \"If youâ€™re running your LLM through langchain your output can be utilised as commands for functions in Python that can use tools. No mcp required. Just tool use\",\n",
      "                    \"score\": 1,\n",
      "                    \"replies\": []\n",
      "                },\n",
      "                {\n",
      "                    \"body\": \"Sure but I think the bigger question is why a separate MCP for Git, LS etc. and not just give the ability to run Bash commands\",\n",
      "                    \"score\": 4,\n",
      "                    \"replies\": [\n",
      "                        {\n",
      "                            \"body\": \"One reason is that the MCP server is self-contained, therefore decoupled from the user's  environment and OS specifics. As an example, a git command execution via shell would fail if git is not installed or is not in the path.\\nAnother is that the MCP layer allows the LLM to operate at the git/version abstraction level not only the shell invocation one, allowing for more precision in terms of selecting the correct tool and its proper usage.\",\n",
      "                            \"score\": 4,\n",
      "                            \"replies\": [\n",
      "                                {\n",
      "                                    \"body\": \"But installing git is easier and more familiar than setting up an MCP server that somehow wraps your own git, and same goes for any other CLI utility.\\n\\nItâ€™s just moving the setup / auth problem into the MCP which makes it more obscure.\",\n",
      "                                    \"score\": 4,\n",
      "                                    \"replies\": [\n",
      "                                        {\n",
      "                                            \"body\": \"Ideally, an MCP server (e.g. git) would be using the primitives via a library not wrapping and invoking an executable. The AuthN concern is certainly valid, however you might want the MCP server to operate read-only for instance, instead of inheriting your potentially superuser privileges.\",\n",
      "                                            \"score\": 1,\n",
      "                                            \"replies\": []\n",
      "                                        }\n",
      "                                    ]\n",
      "                                }\n",
      "                            ]\n",
      "                        }\n",
      "                    ]\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"body\": \"Maybe you are referring to using an AI powered coding tool? Those have AI tools built for reading files, accessing the shell, etc. the power of MCP is you don't need to build a tool that is restricted to a specific AI coding tool, you can just expose the MCP server remotely and any LLM that supports MCP can access it.\",\n",
      "            \"score\": 10,\n",
      "            \"replies\": [\n",
      "                {\n",
      "                    \"body\": \"yes this is it. thanks!\",\n",
      "                    \"score\": 1,\n",
      "                    \"replies\": []\n",
      "                },\n",
      "                {\n",
      "                    \"body\": \"So in your opinion AI powered tool != MCP. And seems OP agrees. Do you see the irony ?\",\n",
      "                    \"score\": 1,\n",
      "                    \"replies\": []\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"body\": \"They're not needed; most clients have built-in tools for filesystem access. \\n\\nFor git, you can just tell the agent to use the \\\"gh\\\" command. Same for anything where a command exists.\",\n",
      "            \"score\": 4,\n",
      "            \"replies\": [\n",
      "                {\n",
      "                    \"body\": \"> For git, you can just tell the agent to use the \\\"gh\\\" command\\n\\ngh does not perform most git actions (add, commit, push, etc.) - instead, it is for github-specific actions which aren't normally handled by the 'git' CLI app.\\n\\n> Same for anything where a command exists.\\n\\nAgents may be able to use CLI commands such as git, yes, but they work better with the git MCP etc.\\n\\n> most clients have built-in tools for filesystem access.\\n\\nYou may be correct about this though.\",\n",
      "                    \"score\": 1,\n",
      "                    \"replies\": []\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"body\": \"if you consider command line an mcp on its own, then that is the only mcp you need. ie if you can run bash. never use git mcp just use cli\\n \\nstill need that first bash mcp. but that's like giving root access to a user so they can browse the web.\",\n",
      "            \"score\": 2,\n",
      "            \"replies\": []\n",
      "        },\n",
      "        {\n",
      "            \"body\": \"Llms can only generate text. With a plugged MCP it can generate text that invokes something and because MCP is freely programmable it can invoke anything.\\n\\nSo: llms cannot strike an atomic bomb. But with MCP they could.\",\n",
      "            \"score\": 1,\n",
      "            \"replies\": []\n",
      "        },\n",
      "        {\n",
      "            \"body\": \"bcs when Augument Code has issue with Terminal commands, you can install desktop commander mcp and keep working.\",\n",
      "            \"score\": 1,\n",
      "            \"replies\": []\n",
      "        },\n",
      "        {\n",
      "            \"body\": \"If the AI tool you're building is going to have access to the shell, and if it can be done via function calling, then you don't need MCPs for basic tools.\\n\\nCase in point, built a file organiser tool for a client recently. Web app that uses the Claude Code SDK to invoke Claude Code to run in the shell of the backend server. Hosted the whole thing on a Secure VM with strict network access. Client uploads files with a set of instructions to rename, regroup etc, web app saves the files to a local directory and invokes Claude Code with the instructions.\\n\\nAnthropic is atrocious at reading PDFs, so I had to use Gemini for it. Initially I configured an MCP to access gemini, but this turned out to be a pain. I had to now run and maintain another server on the VM.\\n\\nSo, I just decided to use the Gemini CLI on the vm. Authenticated it with env variables, printed out \\\\`gemini --help\\\\` output and sprinkled in some additional instructions and added them to the subagent definitions. Now the subagent was able to invoke Gemini with no MCPs.\\n\\nWorked just as good, very little devops overhead. And a surprising added bonus, less token usage.  \\n  \\n So yeah, you shouldn't try to force MCPs into every use case. Sometimes, there are better ways.\",\n",
      "            \"score\": 1,\n",
      "            \"replies\": []\n",
      "        },\n",
      "        {\n",
      "            \"body\": \"Because those that canâ€™t code need ai to do things for them.  Once you have core then you customise to think of mcp as cooypast ai tools to evolve\",\n",
      "            \"score\": 1,\n",
      "            \"replies\": []\n",
      "        },\n",
      "        {\n",
      "            \"body\": \"I just wrote a LinkedIn post today answering this question: \\n\\nðŸ¦¦ Is MCP extra complexity to standardize tools, resources, prompts, and API calls which we could already make directly?\\n\\nWhen using MCP with stdio transport and spinning up servers locally, it is understandable to wonder if MCP is over-engineering, even with automatic tool discoverability.\\n\\nIf we had stop with stdio, then we would have overlooked MCPâ€™s true value as a protocol for remote, reusable context engineering. Say hello to streamable-http transport. By hosting tools on dedicated servers using streamable-http transport, MCP becomes:\\n\\nReusable: Deploy once, use across multiple LM powered applications.\\n\\nScalable: Stream large or long-running results over HTTP for better reliability and user experience.\\n\\nCentralized: Clear ownership ensures maintainability and accountability.\\n\\nFar from redundant over-engineering, MCP provides infrastructure for a shared, evolving ecosystem of context engineering across LLM, RLM and MLLM applications.\\n\\nhttps://www.linkedin.com/posts/prayson_mcp-activity-7371414645569515520-3ukd?utm_source=share&utm_medium=member_ios&rcm=ACoAAAiqLpQBGCf8Bvyfe-IxXSFwqnA_TSRMfMs\",\n",
      "            \"score\": 1,\n",
      "            \"replies\": []\n",
      "        },\n",
      "        {\n",
      "            \"body\": \"I'm curious how you think an LLM can use a tool.\",\n",
      "            \"score\": 1,\n",
      "            \"replies\": [\n",
      "                {\n",
      "                    \"body\": \"There are lots of agent frameworks that have their own ways to build tools. MCP isn't really necessary if you're building an agent and purpose-built tools for it that you don't intend on distributing to a wider audience.\",\n",
      "                    \"score\": 5,\n",
      "                    \"replies\": [\n",
      "                        {\n",
      "                            \"body\": \"Understood. Iâ€™m just not aware of any LLM that does this by itself. It needs some sort of adapter layer.\",\n",
      "                            \"score\": 7,\n",
      "                            \"replies\": [\n",
      "                                {\n",
      "                                    \"body\": \"OP is using a IDE like cursor and is conflating the features of the IDE with the LLM \\n\\nE.g. cursor comes with a terminal tool which can do many things in the terminal without having to use MCP\",\n",
      "                                    \"score\": 2,\n",
      "                                    \"replies\": []\n",
      "                                },\n",
      "                                {\n",
      "                                    \"body\": \"Oh yeah, for sure. LLMs have a part of the API for tool names and descriptions, but all they can do is choose a tool, the application has to actually execute them.\",\n",
      "                                    \"score\": 1,\n",
      "                                    \"replies\": []\n",
      "                                }\n",
      "                            ]\n",
      "                        }\n",
      "                    ]\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pydantic import BaseModel, computed_field\n",
    "from typing import List, Optional\n",
    "\n",
    "class Comment(BaseModel):\n",
    "    body: str\n",
    "    score: int\n",
    "    replies: list['Comment'] = []\n",
    "\n",
    "\n",
    "class Post(BaseModel):\n",
    "    title: str\n",
    "    selftext: str\n",
    "    score: int\n",
    "    url: str\n",
    "    id: str\n",
    "    comments: list[Comment] = []\n",
    "    \n",
    "\n",
    "def extract_comment_tree(comment, reply_mult, score_mult, max_depth=3, current_depth=0) -> Comment:\n",
    "    \"\"\"Recursively extract comment data into a tree structure\"\"\"\n",
    "    if current_depth >= max_depth:\n",
    "        return None\n",
    "    \n",
    "    comment_data = Comment(\n",
    "        body = comment.body,\n",
    "        score = comment.score * score_mult,\n",
    "        created_utc = comment.created_utc,\n",
    "        replies = []\n",
    "    )\n",
    "    \n",
    "    # Process replies if they exist and we haven't reached max depth\n",
    "    if hasattr(comment, 'replies') and comment.replies and current_depth < max_depth - 1:\n",
    "        for reply in comment.replies:\n",
    "            if hasattr(reply, 'body'):  # Make sure it's actually a comment\n",
    "                comment_data.score += reply_mult\n",
    "                reply_data = extract_comment_tree(reply, max_depth, current_depth + 1)\n",
    "                if reply_data:\n",
    "                    comment_data.replies.append(reply_data)\n",
    "    \n",
    "    return comment_data\n",
    "\n",
    "def extract_post_tree(submission, reply_mult, score_mult, max_depth=3) -> Post:\n",
    "    \n",
    "    post_data = Post(\n",
    "        title = submission.title,\n",
    "        selftext = submission.selftext,\n",
    "        score = submission.score * score_mult,\n",
    "        url = submission.url,\n",
    "        id = submission.id,\n",
    "        comments = []\n",
    "    )\n",
    "\n",
    "    # Process top-level comments\n",
    "    submission.comments.replace_more(limit=0)  # Remove \"more comments\" objects\n",
    "    for comment in submission.comments:\n",
    "        if hasattr(comment, 'body'):  # Make sure it's actually a comment\n",
    "            comment_tree = extract_comment_tree(comment, reply_mult, score_mult, max_depth)\n",
    "            if comment_tree:\n",
    "                post_data.comments.append(comment_tree)\n",
    "    \n",
    "    return post_data\n",
    "\n",
    "# Print the JSON structure\n",
    "\n",
    "scored_post = extract_post_tree(submission, 2, 1, 3)\n",
    "print(scored_post.model_dump_json(indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79e6688c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Comments:\n",
      "==================================================\n",
      "1. Score: 9 | Author: PhilWheat\n",
      "   Body: MCP servers aren't NEEDED for that, but by using it, then you can re-use that functionality across c...\n",
      "\n",
      "2. Score: 6 | Author: MurkyCaptain6604\n",
      "   Body: LLMs can generate commands but can't actually execute them locally. They have no access to your loca...\n",
      "\n",
      "3. Score: 6 | Author: Verusauxilium\n",
      "   Body: Maybe you are referring to using an AI powered coding tool? Those have AI tools built for reading fi...\n",
      "\n",
      "4. Score: 2 | Author: jedisct1\n",
      "   Body: They're not needed; most clients have built-in tools for filesystem access. \n",
      "\n",
      "For git, you can just ...\n",
      "\n",
      "5. Score: 2 | Author: Peter-rabbit010\n",
      "   Body: if you consider command line an mcp on its own, then that is the only mcp you need. ie if you can ru...\n",
      "\n",
      "6. Score: 1 | Author: Tobi-Random\n",
      "   Body: Llms can only generate text. With a plugged MCP it can generate text that invokes something and beca...\n",
      "\n",
      "7. Score: 1 | Author: serg33v\n",
      "   Body: bcs when Augument Code has issue with Terminal commands, you can install desktop commander mcp and k...\n",
      "\n",
      "8. Score: 1 | Author: Ravager94\n",
      "   Body: If the AI tool you're building is going to have access to the shell, and if it can be done via funct...\n",
      "\n",
      "9. Score: 1 | Author: fasti-au\n",
      "   Body: Because those that canâ€™t code need ai to do things for them.  Once you have core then you customise ...\n",
      "\n",
      "10. Score: 1 | Author: KitchenFalcon4667\n",
      "   Body: I just wrote a LinkedIn post today answering this question: \n",
      "\n",
      "ðŸ¦¦ Is MCP extra complexity to standardi...\n",
      "\n",
      "\n",
      "Top 10 Replies:\n",
      "==================================================\n",
      "1. Score: 2 | Author: lirantal\n",
      "   Body: this is the right answer, truly. u/rm-rf-rm your fundamental assumption is incorrect - \"LLM ha sno p...\n",
      "\n",
      "2. Score: 2 | Author: AchillesDev\n",
      "   Body: There are lots of agent frameworks that have their own ways to build tools. MCP isn't really necessa...\n",
      "\n",
      "3. Score: 2 | Author: KitchenFalcon4667\n",
      "   Body: u/rm-rf-rm might have `eval {llm_response}` where llm_respose is something like `git log`. In that c...\n",
      "\n",
      "4. Score: 1 | Author: fenixnoctis\n",
      "   Body: Sure but I think the bigger question is why a separate MCP for Git, LS etc. and not just give the ab...\n",
      "\n",
      "5. Score: 1 | Author: GnistAI\n",
      "   Body: Arguably you could equip an LLM with a single tool \"bash\" and it would be all it needs. MCP just mak...\n",
      "\n",
      "6. Score: 1 | Author: James-the-greatest\n",
      "   Body: If youâ€™re running your LLM through langchain your output can be utilised as commands for functions i...\n",
      "\n",
      "7. Score: 1 | Author: MurkyCaptain6604\n",
      "   Body: One reason is that the MCP server is self-contained, therefore decoupled from the user's  environmen...\n",
      "\n",
      "8. Score: 1 | Author: rm-rf-rm\n",
      "   Body: yes this is it. thanks!...\n",
      "\n",
      "9. Score: 1 | Author: Global-Molasses2695\n",
      "   Body: So in your opinion AI powered tool != MCP. And seems OP agrees. Do you see the irony ?...\n",
      "\n",
      "10. Score: 1 | Author: no-name-here\n",
      "   Body: > For git, you can just tell the agent to use the \"gh\" command\n",
      "\n",
      "gh does not perform most git actions...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the list and identify the top X comments\n",
    "# Then iterate over the replies and identify the top Y replies\n",
    "\n",
    "top_n_comments = 10\n",
    "top_n_replies = 10\n",
    "\n",
    "def get_top_comments(comments, n=10):\n",
    "    \"\"\"Get top N comments sorted by score (descending)\"\"\"\n",
    "    return sorted(comments, key=lambda x: x.score + len(x.replies), reverse=True)[:n]\n",
    "\n",
    "def get_all_replies_from_comments(comments):\n",
    "    \"\"\"Recursively collect all replies from a list of comments\"\"\"\n",
    "    all_replies = []\n",
    "\n",
    "    def collect_replies(comment):\n",
    "        for reply in comment.replies:\n",
    "            all_replies.append(reply)\n",
    "            collect_replies(reply)  # Recursively collect nested replies\n",
    "    \n",
    "    for comment in comments:\n",
    "        collect_replies(comment)\n",
    "    \n",
    "    return all_replies\n",
    "\n",
    "def get_top_replies(comments, n=10):\n",
    "    \"\"\"Get top N replies from all replies in the comment tree, sorted by score\"\"\"\n",
    "    all_replies = get_all_replies_from_comments(comments)\n",
    "    return sorted(all_replies, key=lambda x: x.score + len(x.replies), reverse=True)[:n]\n",
    "\n",
    "# Apply the logic to our post data\n",
    "top_comments = get_top_comments(post_data.comments, top_n_comments)\n",
    "top_replies = get_top_replies(post_data.comments, top_n_replies)\n",
    "\n",
    "print(f\"Top {top_n_comments} Comments:\")\n",
    "print(\"=\" * 50)\n",
    "for i, comment in enumerate(top_comments, 1):\n",
    "    print(f\"{i}. Score: {comment.score} | Author: {comment.author}\")\n",
    "    print(f\"   Body: {comment.body[:100]}...\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nTop {top_n_replies} Replies:\")\n",
    "print(\"=\" * 50)\n",
    "for i, reply in enumerate(top_replies, 1):\n",
    "    print(f\"{i}. Score: {reply.score} | Author: {reply.author}\")\n",
    "    print(f\"   Body: {reply.body[:100]}...\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse7319-term-project (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
