{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffdcde6b",
   "metadata": {},
   "source": [
    "Based on the following video: [How-to Use The Reddit API in Python by James Briggs](https://www.youtube.com/watch?v=FdjVoOf9HN4)\n",
    "1. Create a reddit application [here](https://www.reddit.com/prefs/apps) to get a client_id and client_secret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b8e12",
   "metadata": {},
   "source": [
    "# Example workflow\n",
    "\n",
    "## Step 1: Find a Trending Post\n",
    "Use various sorting techniques such as hot, trending, controversial to select a post with insightful information. [Example post](https://www.reddit.com/r/mcp/comments/1nculrw/why_are_mcps_needed_for_basic_tools_like/)\n",
    "\n",
    "## Step 2: Convert the Post Into a YAML Summary\n",
    "Apply a rule-based system to generate a snapshot of the post. Ex:\n",
    "- score each comment: score = upvotes + replys * 10\n",
    "- output the top 10 comments + any parents\n",
    "- generate the yaml to use as input for the llm\n",
    "\n",
    "```yaml\n",
    "post_title: \"Why are MCPs needed for basic tools like filesystem access, git etc?\"\n",
    "post_body: \"Im puzzled why an MCP (that too written in TS) is required for something as basic as reading, listing files etc. In my experience, the LLM has no problem in doing these tasks without any reliability concerns without any MCP. Likewise for git and even gh.\"\n",
    "children:\n",
    "    - comment: \"MCP servers aren't NEEDED for that, but by using it, then you can re-use that functionality across clients vs implementing it each time in the client. That being said, the embedded functionality would likely work faster/better because of the tighter integration. So basically, you pick your sensitivity to effort cost and take the appropriate route.\"\n",
    "    - comment: \"LLMs can generate commands but can't actually execute them locally. They have no access to your local resources (e.g. files, apps). Simply put, MCP bridges that gap so the LLM can actually interact with your real environment instead of you having to copy paste commands back and forth. Additionally, they serve as the building blocks for agentic workflows.\"\n",
    "      children:\n",
    "      - comment: \"this is the right answer, truly. u/rm-rf-rm your fundamental assumption is incorrect - \\\"LLM ha sno problem in doing these\\\" is incorrect. LLMs can say \\\"list directories in ...\\\" but it then has to be a tool call by the MCP client that performs that. the question maybe then becomes - should capabilities like file system be implemented by MCP clients by default or be provided via MCPs.\"\n",
    "      - comment: \"u/rm-rf-rm might have eval {llm_response} where llm_respose is something like git log. In that case s/he might be correct that llm + sh love can do that\"\n",
    "    - comment: \"Sure but I think the bigger question is why a separate MCP for Git, LS etc. and not just give the ability to run Bash commands\"\n",
    "      children:\n",
    "        - comment: \"One reason is that the MCP server is self-contained, therefore decoupled from the user's environment and OS specifics. As an example, a git command execution via shell would fail if git is not installed or is not in the path. Another is that the MCP layer allows the LLM to operate at the git/version abstraction level not only the shell invocation one, allowing for more precision in terms of selecting the correct tool and its proper usage.\"\n",
    "    - comment: \"Maybe you are referring to using an AI powered coding tool? Those have AI tools built for reading files, accessing the shell, etc. the power of MCP is you don't need to build a tool that is restricted to a specific AI coding tool, you can just expose the MCP server remotely and any LLM that supports MCP can access it.\"\n",
    "    - comment: \"They're not needed; most clients have built-in tools for filesystem access. For git, you can just tell the agent to use the \\\"gh\\\" command. Same for anything where a command exists.\"\n",
    "      children:\n",
    "        - comment: \"For git, you can just tell the agent to use the \\\"gh\\\" command gh does not perform most git actions (add, commit, push, etc.) - instead, it is for github-specific actions which aren't normally handled by the 'git' CLI app. Same for anything where a command exists. Agents may be able to use CLI commands such as git, yes, but they work better with the git MCP etc. most clients have built-in tools for filesystem access. You may be correct about this though.\"\n",
    "```\n",
    "\n",
    "## Step 3: Query the LLM:\n",
    "```yaml\n",
    "prompt: \"generate a short linkedin post from this conversation. Extract the debate or lightbulb moment from this thread and present it.\"\n",
    "context: {article_and_comments}\n",
    "```\n",
    "\n",
    "## Step 4: Render the Output\n",
    "\n",
    "**ðŸ”§ The Great MCP Debate: Building Blocks vs. Built-ins**\n",
    "\n",
    "Had a fascinating discussion about why we need Model Context Protocol (MCP) servers for \"basic\" tasks like file operations and git commands. The lightbulb moment? It's not about what LLMs *can* generateâ€”it's about execution and reusability.\n",
    "\n",
    "**The core insight:** LLMs can write perfect bash commands, but they can't actually *run* them. That's where MCPs bridge the gap between AI suggestions and real system interaction.\n",
    "\n",
    "**But here's the real debate:** Should we have specialized MCPs for git, filesystem, etc., or just give LLMs direct shell access?\n",
    "\n",
    "**The case for specialized MCPs:**\n",
    "â€¢ Abstraction over raw commands (git operations vs. shell invocations)  \n",
    "â€¢ Environment-agnostic (works regardless of OS or installed tools)\n",
    "â€¢ Reusable across different AI clients\n",
    "â€¢ Better error handling and context\n",
    "\n",
    "**The case for shell access:**\n",
    "â€¢ Simpler implementation\n",
    "â€¢ Leverages existing tooling\n",
    "â€¢ More flexible for edge cases\n",
    "\n",
    "The thread revealed a fundamental tension in AI tooling: granular, safe abstractions vs. powerful, direct access. Both approaches have merit depending on your use case and risk tolerance.\n",
    "\n",
    "What's your take? Specialized tools or universal shell access for AI agents?\n",
    "\n",
    "#AI #MCP #DeveloperTools #LLM #Automation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00af0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "REDDIT_CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "REDDIT_CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "REDDIT_USERNAME = os.getenv(\"REDDIT_USERNAME\")\n",
    "REDDIT_PASSWORD = os.getenv(\"REDDIT_PASSWORD\")\n",
    "\n",
    "# Authenticate with reddit api\n",
    "reddit = praw.Reddit(\n",
    "    client_id = REDDIT_CLIENT_ID,\n",
    "    client_secret = REDDIT_CLIENT_SECRET,\n",
    "    user_agent = REDDIT_USERNAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab5588b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the example post\n",
    "submission = reddit.submission(id=\"1nculrw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d5d1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the extracted tree of comments into a flattened list, so we can sort it\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, computed_field\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "class Comment(BaseModel):\n",
    "    id: str\n",
    "    author: str\n",
    "    body: str\n",
    "    score: int\n",
    "    created_utc: float\n",
    "    parent_id: Optional[str] = None\n",
    "    depth: int = 0\n",
    "\n",
    "\n",
    "class Post(BaseModel):\n",
    "    title: str\n",
    "    selftext: str\n",
    "    score: int\n",
    "    upvote_ratio: float\n",
    "    num_comments: int\n",
    "    created_utc: float\n",
    "    url: str\n",
    "    id: str\n",
    "    comments: Dict[str, Comment] = {}\n",
    "    \n",
    "    def get_top_comments(self, n: int = 10) -> list[Comment]:\n",
    "        \"\"\"Get top N comments sorted by score descending\"\"\"\n",
    "        return sorted(self.comments.values(), key=lambda x: x.score, reverse=True)[:n]\n",
    "    \n",
    "    def get_top_level_comments(self, n: int = 10) -> list[Comment]:\n",
    "        \"\"\"Get top N top-level comments (no parent) sorted by score\"\"\"\n",
    "        top_level = [c for c in self.comments.values() if c.parent_id is None]\n",
    "        return sorted(top_level, key=lambda x: x.score, reverse=True)[:n]\n",
    "    \n",
    "    def get_replies(self, parent_id: str) -> list[Comment]:\n",
    "        \"\"\"Get all direct replies to a specific comment\"\"\"\n",
    "        return [c for c in self.comments.values() if c.parent_id == parent_id]\n",
    "    \n",
    "    def get_comment(self, comment_id: str) -> Optional[Comment]:\n",
    "        \"\"\"Get a specific comment by ID\"\"\"\n",
    "        return self.comments.get(comment_id)\n",
    "\n",
    "\n",
    "def extract_comments_flat(comment, parent_id=None, depth=0, comment_dict=None, max_depth=3):\n",
    "    \"\"\"Recursively extract comments into a flattened dictionary with parent references\"\"\"\n",
    "    if depth >= max_depth or comment_dict is None:\n",
    "        return\n",
    "    \n",
    "    # Add current comment to the flat dictionary\n",
    "    comment_data = Comment(\n",
    "        id = comment.id,\n",
    "        author = str(comment.author) if comment.author else '[deleted]',\n",
    "        body = comment.body,\n",
    "        score = comment.score,\n",
    "        created_utc = comment.created_utc,\n",
    "        parent_id = parent_id,\n",
    "        depth = depth\n",
    "    )\n",
    "    comment_dict[comment.id] = comment_data\n",
    "    \n",
    "    # Process replies if they exist and we haven't reached max depth\n",
    "    if hasattr(comment, 'replies') and comment.replies and depth < max_depth - 1:\n",
    "        for reply in comment.replies:\n",
    "            if hasattr(reply, 'body'):  # Make sure it's actually a comment\n",
    "                extract_comments_flat(reply, comment.id, depth + 1, comment_dict, max_depth)\n",
    "\n",
    "post_data = Post(\n",
    "    title = submission.title,\n",
    "    selftext = submission.selftext,\n",
    "    score = submission.score,\n",
    "    upvote_ratio = submission.upvote_ratio,\n",
    "    num_comments = submission.num_comments,\n",
    "    created_utc = submission.created_utc,\n",
    "    url = submission.url,\n",
    "    id = submission.id,\n",
    "    comments = {}\n",
    ")\n",
    "\n",
    "# Process top-level comments and flatten them\n",
    "submission.comments.replace_more(limit=0)  # Remove \"more comments\" objects\n",
    "for comment in submission.comments:\n",
    "    if hasattr(comment, 'body'):  # Make sure it's actually a comment\n",
    "        extract_comments_flat(comment, parent_id=None, depth=0, comment_dict=post_data.comments, max_depth=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e6688c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_title: Why are MCPs needed for basic tools like filesystem access, git etc?\n",
      "post_body: 'Im puzzled why an MCP (that too written in TS) is required for something\n",
      "  as basic as reading, listing files etc. In my experience, the LLM has no problem\n",
      "  in doing these tasks without any reliability concerns without any MCP. Likewise\n",
      "  for git and even gh. '\n",
      "post_score: 3\n",
      "children:\n",
      "- comment: 'MCP servers aren''t NEEDED for that, but by using it, then you can re-use\n",
      "    that functionality across clients vs implementing it each time in the client.    That\n",
      "    being said, the embedded functionality would likely work faster/better because\n",
      "    of the tighter integration.\n",
      "\n",
      "\n",
      "    So basically, you pick your sensitivity to effort cost and take the appropriate\n",
      "    route.'\n",
      "  score: 9\n",
      "  author: PhilWheat\n",
      "- comment: LLMs can generate commands but can't actually execute them locally. They\n",
      "    have no access to your local resources (e.g. files, apps). Simply put, MCP bridges\n",
      "    that gap so the LLM can actually interact with your real environment instead of\n",
      "    you having to copy paste commands back and forth. Additionally, they serve as\n",
      "    the building blocks for agentic workflows.\n",
      "  score: 6\n",
      "  author: MurkyCaptain6604\n",
      "  children:\n",
      "  - comment: 'this is the right answer, truly. u/rm-rf-rm your fundamental assumption\n",
      "      is incorrect - \"LLM ha sno problem in doing these\" is incorrect. LLMs can say\n",
      "      \"list directories in ...\" but it then has to be a tool call by the MCP client\n",
      "      that performs that.\n",
      "\n",
      "\n",
      "      the question maybe then becomes - should capabilities like file system be implemented\n",
      "      by MCP clients by default or be provided via MCPs.'\n",
      "    score: 2\n",
      "    author: lirantal\n",
      "    children:\n",
      "    - comment: u/rm-rf-rm might have `eval {llm_response}` where llm_respose is something\n",
      "        like `git log`. In that case s/he might be correct that llm + sh love can\n",
      "        do that\n",
      "      score: 2\n",
      "      author: KitchenFalcon4667\n",
      "    - comment: Arguably you could equip an LLM with a single tool \"bash\" and it would\n",
      "        be all it needs. MCP just makes it more likely that it will succeed.\n",
      "      score: 1\n",
      "      author: GnistAI\n",
      "  - comment: If youâ€™re running your LLM through langchain your output can be utilised\n",
      "      as commands for functions in Python that can use tools. No mcp required. Just\n",
      "      tool use\n",
      "    score: 1\n",
      "    author: James-the-greatest\n",
      "  - comment: Sure but I think the bigger question is why a separate MCP for Git, LS\n",
      "      etc. and not just give the ability to run Bash commands\n",
      "    score: 1\n",
      "    author: fenixnoctis\n",
      "    children:\n",
      "    - comment: 'One reason is that the MCP server is self-contained, therefore decoupled\n",
      "        from the user''s  environment and OS specifics. As an example, a git command\n",
      "        execution via shell would fail if git is not installed or is not in the path.\n",
      "\n",
      "        Another is that the MCP layer allows the LLM to operate at the git/version\n",
      "        abstraction level not only the shell invocation one, allowing for more precision\n",
      "        in terms of selecting the correct tool and its proper usage.'\n",
      "      score: 1\n",
      "      author: MurkyCaptain6604\n",
      "- comment: Maybe you are referring to using an AI powered coding tool? Those have\n",
      "    AI tools built for reading files, accessing the shell, etc. the power of MCP is\n",
      "    you don't need to build a tool that is restricted to a specific AI coding tool,\n",
      "    you can just expose the MCP server remotely and any LLM that supports MCP can\n",
      "    access it.\n",
      "  score: 6\n",
      "  author: Verusauxilium\n",
      "  children:\n",
      "  - comment: yes this is it. thanks!\n",
      "    score: 1\n",
      "    author: rm-rf-rm\n",
      "  - comment: So in your opinion AI powered tool != MCP. And seems OP agrees. Do you\n",
      "      see the irony ?\n",
      "    score: 1\n",
      "    author: Global-Molasses2695\n",
      "- comment: \"They're not needed; most clients have built-in tools for filesystem access.\\\n",
      "    \\ \\n\\nFor git, you can just tell the agent to use the \\\"gh\\\" command. Same for\\\n",
      "    \\ anything where a command exists.\"\n",
      "  score: 2\n",
      "  author: jedisct1\n",
      "  children:\n",
      "  - comment: '> For git, you can just tell the agent to use the \"gh\" command\n",
      "\n",
      "\n",
      "      gh does not perform most git actions (add, commit, push, etc.) - instead, it\n",
      "      is for github-specific actions which aren''t normally handled by the ''git''\n",
      "      CLI app.\n",
      "\n",
      "\n",
      "      > Same for anything where a command exists.\n",
      "\n",
      "\n",
      "      Agents may be able to use CLI commands such as git, yes, but they work better\n",
      "      with the git MCP etc.\n",
      "\n",
      "\n",
      "      > most clients have built-in tools for filesystem access.\n",
      "\n",
      "\n",
      "      You may be correct about this though.'\n",
      "    score: 1\n",
      "    author: no-name-here\n",
      "- comment: \"if you consider command line an mcp on its own, then that is the only\\\n",
      "    \\ mcp you need. ie if you can run bash. never use git mcp just use cli\\n \\nstill\\\n",
      "    \\ need that first bash mcp. but that's like giving root access to a user so they\\\n",
      "    \\ can browse the web.\"\n",
      "  score: 2\n",
      "  author: Peter-rabbit010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the top comments, then compile them into a string for the llm\n",
    "\n",
    "import yaml\n",
    "\n",
    "def generate_llm_content(post: Post, top_n_comments: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Generate LLM-ready content by extracting top N comments and building a hierarchical structure\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get top N comments sorted by score\n",
    "    top_comments = post.get_top_comments(top_n_comments)\n",
    "    \n",
    "    # Build the hierarchical structure\n",
    "    def build_comment_structure(comment: Comment) -> dict:\n",
    "        \"\"\"Build a comment structure with its children\"\"\"\n",
    "        structure = {\n",
    "            \"comment\": comment.body,\n",
    "            \"score\": comment.score,\n",
    "            \"author\": comment.author\n",
    "        }\n",
    "        \n",
    "        # Get direct replies to this comment\n",
    "        replies = post.get_replies(comment.id)\n",
    "        if replies:\n",
    "            # Sort replies by score and add them as children\n",
    "            sorted_replies = sorted(replies, key=lambda x: x.score, reverse=True)\n",
    "            structure[\"children\"] = [build_comment_structure(reply) for reply in sorted_replies]\n",
    "        \n",
    "        return structure\n",
    "    \n",
    "    # Build the main structure\n",
    "    llm_structure = {\n",
    "        \"post_title\": post.title,\n",
    "        \"post_body\": post.selftext,\n",
    "        \"post_score\": post.score,\n",
    "        \"children\": []\n",
    "    }\n",
    "    \n",
    "    # Add top-level comments only (those without parents) from our top N list\n",
    "    top_level_from_top = [c for c in top_comments if c.parent_id is None]\n",
    "    \n",
    "    for comment in top_level_from_top:\n",
    "        comment_structure = build_comment_structure(comment)\n",
    "        llm_structure[\"children\"].append(comment_structure)\n",
    "    \n",
    "    # Convert to YAML and return\n",
    "    return yaml.dump(llm_structure, default_flow_style=False, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "# Generate the LLM content\n",
    "llm_content = generate_llm_content(post_data, top_n_comments=10)\n",
    "print(llm_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse7319-term-project (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
