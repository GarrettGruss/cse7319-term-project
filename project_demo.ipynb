{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffdcde6b",
   "metadata": {},
   "source": [
    "Based on the following video: [How-to Use The Reddit API in Python by James Briggs](https://www.youtube.com/watch?v=FdjVoOf9HN4)\n",
    "1. Create a reddit application [here](https://www.reddit.com/prefs/apps) to get a client_id and client_secret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b8e12",
   "metadata": {},
   "source": [
    "# Example workflow\n",
    "\n",
    "## Step 1: Find a Trending Post\n",
    "Use various sorting techniques such as hot, trending, controversial to select a post with insightful information. [Example post](https://www.reddit.com/r/mcp/comments/1nculrw/why_are_mcps_needed_for_basic_tools_like/)\n",
    "\n",
    "## Step 2: Convert the Post Into a YAML Summary\n",
    "Apply a rule-based system to generate a snapshot of the post. Ex:\n",
    "- score each comment: score = upvotes + replys * 10\n",
    "- output the top 10 comments + any parents\n",
    "- generate the yaml to use as input for the llm\n",
    "\n",
    "```yaml\n",
    "post_title: \"Why are MCPs needed for basic tools like filesystem access, git etc?\"\n",
    "post_body: \"Im puzzled why an MCP (that too written in TS) is required for something as basic as reading, listing files etc. In my experience, the LLM has no problem in doing these tasks without any reliability concerns without any MCP. Likewise for git and even gh.\"\n",
    "children:\n",
    "    - comment: \"MCP servers aren't NEEDED for that, but by using it, then you can re-use that functionality across clients vs implementing it each time in the client. That being said, the embedded functionality would likely work faster/better because of the tighter integration. So basically, you pick your sensitivity to effort cost and take the appropriate route.\"\n",
    "    - comment: \"LLMs can generate commands but can't actually execute them locally. They have no access to your local resources (e.g. files, apps). Simply put, MCP bridges that gap so the LLM can actually interact with your real environment instead of you having to copy paste commands back and forth. Additionally, they serve as the building blocks for agentic workflows.\"\n",
    "      children:\n",
    "      - comment: \"this is the right answer, truly. u/rm-rf-rm your fundamental assumption is incorrect - \\\"LLM ha sno problem in doing these\\\" is incorrect. LLMs can say \\\"list directories in ...\\\" but it then has to be a tool call by the MCP client that performs that. the question maybe then becomes - should capabilities like file system be implemented by MCP clients by default or be provided via MCPs.\"\n",
    "      - comment: \"u/rm-rf-rm might have eval {llm_response} where llm_respose is something like git log. In that case s/he might be correct that llm + sh love can do that\"\n",
    "    - comment: \"Sure but I think the bigger question is why a separate MCP for Git, LS etc. and not just give the ability to run Bash commands\"\n",
    "      children:\n",
    "        - comment: \"One reason is that the MCP server is self-contained, therefore decoupled from the user's environment and OS specifics. As an example, a git command execution via shell would fail if git is not installed or is not in the path. Another is that the MCP layer allows the LLM to operate at the git/version abstraction level not only the shell invocation one, allowing for more precision in terms of selecting the correct tool and its proper usage.\"\n",
    "    - comment: \"Maybe you are referring to using an AI powered coding tool? Those have AI tools built for reading files, accessing the shell, etc. the power of MCP is you don't need to build a tool that is restricted to a specific AI coding tool, you can just expose the MCP server remotely and any LLM that supports MCP can access it.\"\n",
    "    - comment: \"They're not needed; most clients have built-in tools for filesystem access. For git, you can just tell the agent to use the \\\"gh\\\" command. Same for anything where a command exists.\"\n",
    "      children:\n",
    "        - comment: \"For git, you can just tell the agent to use the \\\"gh\\\" command gh does not perform most git actions (add, commit, push, etc.) - instead, it is for github-specific actions which aren't normally handled by the 'git' CLI app. Same for anything where a command exists. Agents may be able to use CLI commands such as git, yes, but they work better with the git MCP etc. most clients have built-in tools for filesystem access. You may be correct about this though.\"\n",
    "```\n",
    "\n",
    "## Step 3: Query the LLM:\n",
    "```yaml\n",
    "prompt: \"generate a short linkedin post from this conversation. Extract the debate or lightbulb moment from this thread and present it.\"\n",
    "context: {article_and_comments}\n",
    "```\n",
    "\n",
    "## Step 4: Render the Output\n",
    "\n",
    "**ðŸ”§ The Great MCP Debate: Building Blocks vs. Built-ins**\n",
    "\n",
    "Had a fascinating discussion about why we need Model Context Protocol (MCP) servers for \"basic\" tasks like file operations and git commands. The lightbulb moment? It's not about what LLMs *can* generateâ€”it's about execution and reusability.\n",
    "\n",
    "**The core insight:** LLMs can write perfect bash commands, but they can't actually *run* them. That's where MCPs bridge the gap between AI suggestions and real system interaction.\n",
    "\n",
    "**But here's the real debate:** Should we have specialized MCPs for git, filesystem, etc., or just give LLMs direct shell access?\n",
    "\n",
    "**The case for specialized MCPs:**\n",
    "â€¢ Abstraction over raw commands (git operations vs. shell invocations)  \n",
    "â€¢ Environment-agnostic (works regardless of OS or installed tools)\n",
    "â€¢ Reusable across different AI clients\n",
    "â€¢ Better error handling and context\n",
    "\n",
    "**The case for shell access:**\n",
    "â€¢ Simpler implementation\n",
    "â€¢ Leverages existing tooling\n",
    "â€¢ More flexible for edge cases\n",
    "\n",
    "The thread revealed a fundamental tension in AI tooling: granular, safe abstractions vs. powerful, direct access. Both approaches have merit depending on your use case and risk tolerance.\n",
    "\n",
    "What's your take? Specialized tools or universal shell access for AI agents?\n",
    "\n",
    "#AI #MCP #DeveloperTools #LLM #Automation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00af0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "REDDIT_CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "REDDIT_CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "REDDIT_USERNAME = os.getenv(\"REDDIT_USERNAME\")\n",
    "REDDIT_PASSWORD = os.getenv(\"REDDIT_PASSWORD\")\n",
    "\n",
    "# Authenticate with reddit api\n",
    "reddit = praw.Reddit(\n",
    "    client_id = REDDIT_CLIENT_ID,\n",
    "    client_secret = REDDIT_CLIENT_SECRET,\n",
    "    user_agent = REDDIT_USERNAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ab5588b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the example post\n",
    "submission = reddit.submission(id=\"1nculrw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20d5d1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the extracted tree of comments into a flattened list, so we can sort it\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, computed_field\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "class Comment(BaseModel):\n",
    "    id: str\n",
    "    author: str\n",
    "    body: str\n",
    "    score: int\n",
    "    created_utc: float\n",
    "    parent_id: Optional[str] = None\n",
    "    depth: int = 0\n",
    "\n",
    "\n",
    "class Post(BaseModel):\n",
    "    title: str\n",
    "    selftext: str\n",
    "    score: int\n",
    "    upvote_ratio: float\n",
    "    num_comments: int\n",
    "    created_utc: float\n",
    "    url: str\n",
    "    id: str\n",
    "    comments: Dict[str, Comment] = {}\n",
    "    \n",
    "    def get_top_comments(self, n: int = 10) -> list[Comment]:\n",
    "        \"\"\"Get top N comments sorted by score descending\"\"\"\n",
    "        return sorted(self.comments.values(), key=lambda x: x.score, reverse=True)[:n]\n",
    "    \n",
    "    def get_top_level_comments(self, n: int = 10) -> list[Comment]:\n",
    "        \"\"\"Get top N top-level comments (no parent) sorted by score\"\"\"\n",
    "        top_level = [c for c in self.comments.values() if c.parent_id is None]\n",
    "        return sorted(top_level, key=lambda x: x.score, reverse=True)[:n]\n",
    "    \n",
    "    def get_replies(self, parent_id: str) -> list[Comment]:\n",
    "        \"\"\"Get all direct replies to a specific comment\"\"\"\n",
    "        return [c for c in self.comments.values() if c.parent_id == parent_id]\n",
    "    \n",
    "    def get_comment(self, comment_id: str) -> Optional[Comment]:\n",
    "        \"\"\"Get a specific comment by ID\"\"\"\n",
    "        return self.comments.get(comment_id)\n",
    "\n",
    "\n",
    "def extract_comments_flat(comment, parent_id=None, depth=0, comment_dict=None, max_depth=3):\n",
    "    \"\"\"Recursively extract comments into a flattened dictionary with parent references\"\"\"\n",
    "    if depth >= max_depth or comment_dict is None:\n",
    "        return\n",
    "    \n",
    "    # Add current comment to the flat dictionary\n",
    "    comment_data = Comment(\n",
    "        id = comment.id,\n",
    "        author = str(comment.author) if comment.author else '[deleted]',\n",
    "        body = comment.body,\n",
    "        score = comment.score,\n",
    "        created_utc = comment.created_utc,\n",
    "        parent_id = parent_id,\n",
    "        depth = depth\n",
    "    )\n",
    "    comment_dict[comment.id] = comment_data\n",
    "    \n",
    "    # Process replies if they exist and we haven't reached max depth\n",
    "    if hasattr(comment, 'replies') and comment.replies and depth < max_depth - 1:\n",
    "        for reply in comment.replies:\n",
    "            if hasattr(reply, 'body'):  # Make sure it's actually a comment\n",
    "                extract_comments_flat(reply, comment.id, depth + 1, comment_dict, max_depth)\n",
    "\n",
    "post_data = Post(\n",
    "    title = submission.title,\n",
    "    selftext = submission.selftext,\n",
    "    score = submission.score,\n",
    "    upvote_ratio = submission.upvote_ratio,\n",
    "    num_comments = submission.num_comments,\n",
    "    created_utc = submission.created_utc,\n",
    "    url = submission.url,\n",
    "    id = submission.id,\n",
    "    comments = {}\n",
    ")\n",
    "\n",
    "# Process top-level comments and flatten them\n",
    "submission.comments.replace_more(limit=0)  # Remove \"more comments\" objects\n",
    "for comment in submission.comments:\n",
    "    if hasattr(comment, 'body'):  # Make sure it's actually a comment\n",
    "        extract_comments_flat(comment, parent_id=None, depth=0, comment_dict=post_data.comments, max_depth=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e6688c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated YAML content:\n",
      "==================================================\n",
      "post_title: Why are MCPs needed for basic tools like filesystem access, git etc?\n",
      "post_body: 'Im puzzled why an MCP (that too written in TS) is required for something\n",
      "  as basic as reading, listing files etc. In my experience, the LLM has no problem\n",
      "  in doing these tasks without any reliability concerns without any MCP. Likewise\n",
      "  for git and even gh. '\n",
      "post_score: 2\n",
      "children:\n",
      "- comment: 'MCP servers aren''t NEEDED for that, but by using it, then you can re-use\n",
      "    that functionality across clients vs implementing it each time in the client.    That\n",
      "    being said, the embedded functionality would likely work faster/better because\n",
      "    of the tighter integration.\n",
      "\n",
      "\n",
      "    So basically, you pick your sensitivity to effort cost and take the appropriate\n",
      "    route.'\n",
      "  score: 9\n",
      "  author: PhilWheat\n",
      "- comment: LLMs can generate commands but can't actually execute them locally. They\n",
      "    have no access to your local resources (e.g. files, apps). Simply put, MCP bridges\n",
      "    that gap so the LLM can actually interact with your real environment instead of\n",
      "    you having to copy paste commands back and forth. Additionally, they serve as\n",
      "    the building blocks for agentic workflows.\n",
      "  score: 6\n",
      "  author: MurkyCaptain6604\n",
      "  children:\n",
      "  - comment: 'this is the right answer, truly. u/rm-rf-rm your fundamental assumption\n",
      "      is incorrect - \"LLM ha sno problem in doing these\" is incorrect. LLMs can say\n",
      "      \"list directories in ...\" but it then has to be a tool call by the MCP client\n",
      "      that performs that.\n",
      "\n",
      "\n",
      "      the question maybe then becomes - should capabilities like file system be implemented\n",
      "      by MCP clients by default or be provided via MCPs.'\n",
      "    score: 2\n",
      "    author: lirantal\n",
      "    children:\n",
      "    - comment: u/rm-rf-rm might have `eval {llm_response}` where llm_respose is something\n",
      "        like `git log`. In that case s/he might be correct that llm + sh love can\n",
      "        do that\n",
      "      score: 2\n",
      "      author: KitchenFalcon4667\n",
      "    - comment: Arguably you could equip an LLM with a single tool \"bash\" and it would\n",
      "        be all it needs. MCP just makes it more likely that it will succeed.\n",
      "      score: 1\n",
      "      author: GnistAI\n",
      "  - comment: If youâ€™re running your LLM through langchain your output can be utilised\n",
      "      as commands for functions in Python that can use tools. No mcp required. Just\n",
      "      tool use\n",
      "    score: 1\n",
      "    author: James-the-greatest\n",
      "  - comment: Sure but I think the bigger question is why a separate MCP for Git, LS\n",
      "      etc. and not just give the ability to run Bash commands\n",
      "    score: 1\n",
      "    author: fenixnoctis\n",
      "    children:\n",
      "    - comment: 'One reason is that the MCP server is self-contained, therefore decoupled\n",
      "        from the user''s  environment and OS specifics. As an example, a git command\n",
      "        execution via shell would fail if git is not installed or is not in the path.\n",
      "\n",
      "        Another is that the MCP layer allows the LLM to operate at the git/version\n",
      "        abstraction level not only the shell invocation one, allowing for more precision\n",
      "        in terms of selecting the correct tool and its proper usage.'\n",
      "      score: 1\n",
      "      author: MurkyCaptain6604\n",
      "- comment: Maybe you are referring to using an AI powered coding tool? Those have\n",
      "    AI tools built for reading files, accessing the shell, etc. the power of MCP is\n",
      "    you don't need to build a tool that is restricted to a specific AI coding tool,\n",
      "    you can just expose the MCP server remotely and any LLM that supports MCP can\n",
      "    access it.\n",
      "  score: 5\n",
      "  author: Verusauxilium\n",
      "  children:\n",
      "  - comment: yes this is it. thanks!\n",
      "    score: 1\n",
      "    author: rm-rf-rm\n",
      "  - comment: So in your opinion AI powered tool != MCP. And seems OP agrees. Do you\n",
      "      see the irony ?\n",
      "    score: 1\n",
      "    author: Global-Molasses2695\n",
      "- comment: \"They're not needed; most clients have built-in tools for filesystem access.\\\n",
      "    \\ \\n\\nFor git, you can just tell the agent to use the \\\"gh\\\" command. Same for\\\n",
      "    \\ anything where a command exists.\"\n",
      "  score: 2\n",
      "  author: jedisct1\n",
      "  children:\n",
      "  - comment: '> For git, you can just tell the agent to use the \"gh\" command\n",
      "\n",
      "\n",
      "      gh does not perform most git actions (add, commit, push, etc.) - instead, it\n",
      "      is for github-specific actions which aren''t normally handled by the ''git''\n",
      "      CLI app.\n",
      "\n",
      "\n",
      "      > Same for anything where a command exists.\n",
      "\n",
      "\n",
      "      Agents may be able to use CLI commands such as git, yes, but they work better\n",
      "      with the git MCP etc.\n",
      "\n",
      "\n",
      "      > most clients have built-in tools for filesystem access.\n",
      "\n",
      "\n",
      "      You may be correct about this though.'\n",
      "    score: 1\n",
      "    author: no-name-here\n",
      "- comment: \"if you consider command line an mcp on its own, then that is the only\\\n",
      "    \\ mcp you need. ie if you can run bash. never use git mcp just use cli\\n \\nstill\\\n",
      "    \\ need that first bash mcp. but that's like giving root access to a user so they\\\n",
      "    \\ can browse the web.\"\n",
      "  score: 2\n",
      "  author: Peter-rabbit010\n",
      "\n",
      "\n",
      "Querying Gemini API...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758317039.171652  179267 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised NotFound: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised NotFound: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised NotFound: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised NotFound: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     gemini_response = \u001b[43mquery_gemini_with_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mllm_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGenerate a short LinkedIn post from this conversation. Extract the debate or lightbulb moment from this thread and present it.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGemini Response:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mprint\u001b[39m(gemini_response)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mquery_gemini_with_content\u001b[39m\u001b[34m(yaml_content, prompt)\u001b[39m\n\u001b[32m     70\u001b[39m full_prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mContext:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00myaml_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Query the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/garre/development/cse7319-term-project/.venv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:1676\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   1673\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.code_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1674\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1676\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/garre/development/cse7319-term-project/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:395\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m     **kwargs: Any,\n\u001b[32m    391\u001b[39m ) -> BaseMessage:\n\u001b[32m    392\u001b[39m     config = ensure_config(config)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    405\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/garre/development/cse7319-term-project/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1023\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1014\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1020\u001b[39m     **kwargs: Any,\n\u001b[32m   1021\u001b[39m ) -> LLMResult:\n\u001b[32m   1022\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/garre/development/cse7319-term-project/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:840\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    838\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    839\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m840\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    846\u001b[39m         )\n\u001b[32m    847\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    848\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/garre/development/cse7319-term-project/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1089\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1087\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1093\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/garre/development/cse7319-term-project/.venv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:1790\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m   1789\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.max_retries\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1795\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1796\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/garre/development/cse7319-term-project/.venv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:238\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    231\u001b[39m params = (\n\u001b[32m    232\u001b[39m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service}\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (request := kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[32m    237\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/garre/development/cse7319-term-project/.venv/lib/python3.12/site-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/garre/development/cse7319-term-project/.venv/lib/python3.12/site-packages/tenacity/__init__.py:487\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[32m    486\u001b[39m     retry_state.prepare_for_next_attempt()\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/garre/development/cse7319-term-project/.venv/lib/python3.12/site-packages/tenacity/nap.py:31\u001b[39m, in \u001b[36msleep\u001b[39m\u001b[34m(seconds)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     26\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[32m     28\u001b[39m \n\u001b[32m     29\u001b[39m \u001b[33;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "def generate_llm_content(post: Post, top_n_comments: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Generate LLM-ready content by extracting top N comments and building a hierarchical structure\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get top N comments sorted by score\n",
    "    top_comments = post.get_top_comments(top_n_comments)\n",
    "    \n",
    "    # Build the hierarchical structure\n",
    "    def build_comment_structure(comment: Comment) -> dict:\n",
    "        \"\"\"Build a comment structure with its children\"\"\"\n",
    "        structure = {\n",
    "            \"comment\": comment.body,\n",
    "            \"score\": comment.score,\n",
    "            \"author\": comment.author\n",
    "        }\n",
    "        \n",
    "        # Get direct replies to this comment\n",
    "        replies = post.get_replies(comment.id)\n",
    "        if replies:\n",
    "            # Sort replies by score and add them as children\n",
    "            sorted_replies = sorted(replies, key=lambda x: x.score, reverse=True)\n",
    "            structure[\"children\"] = [build_comment_structure(reply) for reply in sorted_replies]\n",
    "        \n",
    "        return structure\n",
    "    \n",
    "    # Build the main structure\n",
    "    llm_structure = {\n",
    "        \"post_title\": post.title,\n",
    "        \"post_body\": post.selftext,\n",
    "        \"post_score\": post.score,\n",
    "        \"children\": []\n",
    "    }\n",
    "    \n",
    "    # Add top-level comments only (those without parents) from our top N list\n",
    "    top_level_from_top = [c for c in top_comments if c.parent_id is None]\n",
    "    \n",
    "    for comment in top_level_from_top:\n",
    "        comment_structure = build_comment_structure(comment)\n",
    "        llm_structure[\"children\"].append(comment_structure)\n",
    "    \n",
    "    # Convert to YAML and return\n",
    "    return yaml.dump(llm_structure, default_flow_style=False, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "def query_gemini_with_content(yaml_content: str, prompt: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Query Google Gemini API with the YAML content using LangChain\n",
    "    \"\"\"\n",
    "    # Get API key from environment\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"GEMINI_API_KEY environment variable not found\")\n",
    "    \n",
    "    # Initialize the Gemini model\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        google_api_key=api_key,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    # Default prompt if none provided\n",
    "    if not prompt:\n",
    "        prompt = \"Generate a short LinkedIn post from this conversation. Extract the debate or lightbulb moment from this thread and present it.\"\n",
    "    \n",
    "    # Combine prompt with YAML content\n",
    "    full_prompt = f\"{prompt}\\n\\nContext:\\n{yaml_content}\"\n",
    "    \n",
    "    # Query the model\n",
    "    response = llm.invoke(full_prompt)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "# Generate the LLM content\n",
    "llm_content = generate_llm_content(post_data, top_n_comments=10)\n",
    "print(\"Generated YAML content:\")\n",
    "print(\"=\" * 50)\n",
    "print(llm_content)\n",
    "\n",
    "# Query Gemini with the content\n",
    "print(\"\\nQuerying Gemini API...\")\n",
    "print(\"=\" * 50)\n",
    "try:\n",
    "    gemini_response = query_gemini_with_content(\n",
    "        llm_content, \n",
    "        \"Generate a short LinkedIn post from this conversation. Extract the debate or lightbulb moment from this thread and present it.\"\n",
    "    )\n",
    "    print(\"Gemini Response:\")\n",
    "    print(gemini_response)\n",
    "except Exception as e:\n",
    "    print(f\"Error querying Gemini: {e}\")\n",
    "    print(\"Make sure GEMINI_API_KEY is set in your environment and langchain-google-genai is installed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse7319-term-project (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
